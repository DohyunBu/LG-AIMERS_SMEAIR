{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import xgboost as xgb\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID별 일일판매량 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train.csv\") \n",
    "\n",
    "# ['제품', '쇼핑몰'] 기준으로 groupby\n",
    "grouped_data1 = train.groupby(['제품','쇼핑몰'])\n",
    "grouped_dfs1 = [group_df for _, group_df in grouped_data1]\n",
    "\n",
    "# 동일한 쇼핑몰에서 판매하는 동일 제품들의 group index 찾음.\n",
    "list1 = []\n",
    "for i in tqdm(range(28863)):\n",
    "    if len(grouped_dfs1[i]) != 1:\n",
    "        list1.append(i)\n",
    "\n",
    "# 동일한 쇼핑몰에서 판매하는 동일 제품들의 'ID'들끼리 일일판매량을 합친 후, 해당 'ID'의 일일판매량으로 대체함.\n",
    "for i in tqdm(list1):\n",
    "    a = pd.DataFrame(np.array(grouped_dfs1[i].iloc[0,7:]) + np.array(grouped_dfs1[i].iloc[1,7:]))\n",
    "    k1 = pd.concat([grouped_dfs1[i].iloc[0,:7].reset_index(drop=True),a],axis=0,ignore_index=True)\n",
    "    k2 = pd.concat([grouped_dfs1[i].iloc[1,:7].reset_index(drop=True),a],axis=0,ignore_index=True)\n",
    "    train.loc[train['ID']==k1.loc[0].tolist()[0]] = list(np.array(k1).reshape(-1,486))\n",
    "    train.loc[train['ID']==k2.loc[0].tolist()[0]] = list(np.array(k2).reshape(-1,486))\n",
    "\n",
    "train.to_csv(\"./train1.csv\",index=False)\n",
    "\n",
    "# 쇼핑몰 4와 10에서 판매한 공통 제품들의 일일판매량을 합친 후, 해당 '제품'의 일일판매량으로 대체함. \n",
    "shop4 = train[train['쇼핑몰']=='S001-00004']\n",
    "shop10 = train[train['쇼핑몰']=='S001-00010']\n",
    "common = list(set(shop10['제품'].tolist())-(set(shop10['제품'].tolist())-set(shop4['제품'].tolist())))\n",
    "for i in tqdm(common):\n",
    "    a = pd.DataFrame(np.array(shop10[shop10['제품']==i].iloc[:,7:]) + np.array(shop4[shop4['제품']==i].iloc[:,7:]))\n",
    "    k1 = pd.concat([shop10[shop10['제품']==i].iloc[:,:7].reset_index(drop=True),a],axis=1,ignore_index=True)\n",
    "    k2 = pd.concat([shop4[shop4['제품']==i].iloc[:,:7].reset_index(drop=True),a],axis=1,ignore_index=True)\n",
    "    shop10.loc[shop10['제품']==i] = list(np.array(k1))\n",
    "    shop4.loc[shop4['제품']==i] = list(np.array(k2))\n",
    "\n",
    "train[train['쇼핑몰']=='S001-00004'] = shop4\n",
    "train[train['쇼핑몰']=='S001-00010'] = shop10\n",
    "\n",
    "train.to_csv(\"./train2.csv\",index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID별 일일판매액 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(\"./sales.csv\")\n",
    "\n",
    "# ['제품', '쇼핑몰'] 기준으로 groupby\n",
    "grouped_data2 = sales.groupby(['제품','쇼핑몰'])\n",
    "grouped_dfs2 = [group_df for _, group_df in grouped_data2]\n",
    "\n",
    "# 동일한 쇼핑몰에서 판매하는 동일 제품들의 group index 찾음.\n",
    "list2 = []\n",
    "for i in tqdm(range(28863)):\n",
    "    if len(grouped_dfs2[i]) != 1:\n",
    "        list2.append(i)\n",
    "        \n",
    "# 동일한 쇼핑몰에서 판매하는 동일 제품들의 'ID'들끼리 일일판매액을 합친 후, 해당 'ID'의 일일판매액으로 대체함.\n",
    "for i in tqdm(list2):\n",
    "    a = pd.DataFrame(np.array(grouped_dfs2[i].iloc[0,7:]) + np.array(grouped_dfs2[i].iloc[1,7:]))\n",
    "    k1 = pd.concat([grouped_dfs2[i].iloc[0,:7].reset_index(drop=True),a],axis=0,ignore_index=True)\n",
    "    k2 = pd.concat([grouped_dfs2[i].iloc[1,:7].reset_index(drop=True),a],axis=0,ignore_index=True)\n",
    "    sales.loc[train['ID']==k1.loc[0].tolist()[0]] = list(np.array(k1).reshape(-1,486))\n",
    "    sales.loc[train['ID']==k2.loc[0].tolist()[0]] = list(np.array(k2).reshape(-1,486))\n",
    "\n",
    "sales.to_csv(\"./sales1.csv\",index=False)\n",
    "\n",
    "# 쇼핑몰 4와 10에서 판매한 공통 제품들의 일일판매액을 합친 후, 해당 '제품'의 일일판매액으로 대체함. \n",
    "sales1 = pd.read_csv(\"./sales1.csv\")\n",
    "shop4 = sales1[sales1['쇼핑몰']=='S001-00004']\n",
    "shop10 = sales1[sales1['쇼핑몰']=='S001-00010']\n",
    "common = list(set(shop10['제품'].tolist())-(set(shop10['제품'].tolist())-set(shop4['제품'].tolist())))\n",
    "for i in tqdm(common):\n",
    "    a = pd.DataFrame(np.array(shop10[shop10['제품']==i].iloc[:,7:]) + np.array(shop4[shop4['제품']==i].iloc[:,7:]))\n",
    "    k1 = pd.concat([shop10[shop10['제품']==i].iloc[:,:7].reset_index(drop=True),a],axis=1,ignore_index=True)\n",
    "    k2 = pd.concat([shop4[shop4['제품']==i].iloc[:,:7].reset_index(drop=True),a],axis=1,ignore_index=True)\n",
    "    shop10.loc[shop10['제품']==i] = list(np.array(k1))\n",
    "    shop4.loc[shop4['제품']==i] = list(np.array(k2))\n",
    "\n",
    "sales1[sales1['쇼핑몰']=='S001-00004'] = shop4\n",
    "sales1[sales1['쇼핑몰']=='S001-00010'] = shop10\n",
    "\n",
    "sales1.to_csv(\"./sales2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대/중/소분류/쇼핑몰 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./train1.csv\") \n",
    "df999=df.copy()\n",
    "df1=df.groupby(\"대분류\").sum()\n",
    "df2= df1.drop([\"ID\",'제품','중분류','소분류','브랜드','쇼핑몰'],axis=1)\n",
    "df3= df2.transpose()\n",
    "\n",
    "# 상관계수 행렬 계산\n",
    "correlation_matrix = df3.corr()\n",
    "\n",
    "# 거리 계산\n",
    "distance_matrix = 1 - correlation_matrix\n",
    "\n",
    "# 다차원 척도법(MDS)를 사용하여 1차원으로 축소\n",
    "mds = MDS(dissimilarity=\"precomputed\", n_components=1,random_state=1)\n",
    "brand_positions = mds.fit_transform(distance_matrix)\n",
    "\n",
    "df2[\"MDS\"]=brand_positions\n",
    "df2=df2.reset_index()\n",
    "df21=df2.iloc[:,[0,-1]]\n",
    "\n",
    "# 대분류 -> 대분류 MDS\n",
    "df99=pd.merge(df,df21,on=\"대분류\")\n",
    "df99=df99.sort_values(\"ID\").reset_index(drop=True)\n",
    "df999['대분류']=df99[\"MDS\"]\n",
    "\n",
    "# 중분류\n",
    "df1_1=df.groupby([\"대분류\",\"중분류\"]).sum()\n",
    "df2_1=df1_1.drop([\"ID\",'제품','소분류','브랜드','쇼핑몰'],axis=1)\n",
    "df3_1= df2_1.transpose()\n",
    "\n",
    "# 상관계수 행렬 계산\n",
    "correlation_matrix_1 = df3_1.corr()\n",
    "\n",
    "# 거리 계산\n",
    "distance_matrix1 = 1 - correlation_matrix_1\n",
    "\n",
    "# 다차원 척도법(MDS)를 사용하여 1차원으로 축소\n",
    "brand_positions1 = mds.fit_transform(distance_matrix1)\n",
    "\n",
    "df2_1[\"MDS\"]=brand_positions1\n",
    "df2_1=df2_1.reset_index()\n",
    "df21_1=df2_1.iloc[:,[0,1,-1]]\n",
    "\n",
    "# 중분류 -> 중분류 MDS\n",
    "df99_1=pd.merge(df,df21_1,on=[\"대분류\",\"중분류\"])\n",
    "df99_1=df99_1.sort_values(\"ID\").reset_index(drop=True)\n",
    "df999['중분류']=df99_1[\"MDS\"]\n",
    "\n",
    "# 소분류\n",
    "df1_2=df.groupby(\"소분류\").sum()\n",
    "df2_2=df1_2.drop([\"ID\",'제품','대분류','중분류','브랜드','쇼핑몰'],axis=1)\n",
    "df3_2= df2_2.transpose()\n",
    "\n",
    "# 상관계수 행렬 계산\n",
    "correlation_matrix_2 = df3_2.corr()\n",
    "\n",
    "distance_matrix2 = 1 - correlation_matrix_2\n",
    "\n",
    "# 다차원 척도법(MDS)를 사용하여 1차원으로 축소\n",
    "brand_positions2 = mds.fit_transform(distance_matrix2)\n",
    "\n",
    "df2_2[\"MDS\"]=brand_positions2\n",
    "df2_2=df2_2.reset_index()\n",
    "df21_2=df2_2.iloc[:,[0,-1]]\n",
    "\n",
    "# 소분류 -> 소분류 MDS\n",
    "df99_2=pd.merge(df,df21_2,on=\"소분류\")\n",
    "df99_2=df99_2.sort_values(\"ID\").reset_index(drop=True)\n",
    "df999['소분류']=df99_2[\"MDS\"]\n",
    "\n",
    "\n",
    "#쇼핑몰\n",
    "df1_3=df.groupby(\"쇼핑몰\").sum()\n",
    "df2_3=df1_3.drop([\"ID\",'제품','대분류','중분류','소분류','브랜드'],axis=1)\n",
    "df3_3= df2_3.transpose()\n",
    "\n",
    "# 상관계수 행렬 계산\n",
    "correlation_matrix_3 = df3_3.corr()\n",
    "\n",
    "distance_matrix3 = 1 - correlation_matrix_3\n",
    "\n",
    "# 다차원 척도법(MDS)를 사용하여 1차원으로 축소\n",
    "brand_positions3 = mds.fit_transform(distance_matrix3)\n",
    "\n",
    "df2_3[\"MDS\"]=brand_positions3\n",
    "df2_3=df2_3.reset_index()\n",
    "df21_3=df2_3.iloc[:,[0,-1]]\n",
    "\n",
    "# 소분류 -> 소분류 MDS\n",
    "df99_3=pd.merge(df,df21_3,on=\"쇼핑몰\")\n",
    "df99_3=df99_3.sort_values(\"ID\").reset_index(drop=True)\n",
    "df999['쇼핑몰']=df99_3[\"MDS\"]\n",
    "\n",
    "df999.to_csv('./train_MDS_train1ver.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일일 개당 판매가 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(\"./sales2.csv\")\n",
    "del sales['대분류']\n",
    "del sales['중분류']\n",
    "del sales['소분류']\n",
    "del sales['쇼핑몰']\n",
    "\n",
    "brand_name = sales['브랜드'].tolist()\n",
    "product_name = sales['제품'].tolist()\n",
    "id_name = sales['ID'].tolist()\n",
    "del sales['ID']\n",
    "del sales['제품']\n",
    "del sales['브랜드']\n",
    "\n",
    "sales = sales.transpose()\n",
    "\n",
    "full_data = []\n",
    "\n",
    "for product_i in tqdm(range(len(id_name))):\n",
    "    y = sales[product_i].tolist()\n",
    "    data = [[i,y[i],product_name[product_i],brand_name[product_i],id_name[product_i]] for i in range(len(y))]\n",
    "    full_data += data\n",
    "price = pd.DataFrame(full_data, columns=['x','y','product','brand','ID'])\n",
    "\n",
    "train = pd.read_csv(\"./train2.csv\")\n",
    "del train['대분류']\n",
    "del train['중분류']\n",
    "del train['소분류']\n",
    "del train['쇼핑몰']\n",
    "\n",
    "brand_name = train['브랜드'].tolist()\n",
    "product_name = train['제품'].tolist()\n",
    "id_name = train['ID'].tolist()\n",
    "del train['제품']\n",
    "del train['브랜드']\n",
    "del train['ID']\n",
    "\n",
    "train = train.transpose()\n",
    "\n",
    "full_data = []\n",
    "\n",
    "for product_i in tqdm(range(len(id_name))):\n",
    "    y = train[product_i].tolist()\n",
    "    data = [[i,y[i],product_name[product_i],brand_name[product_i],id_name[product_i]] for i in range(len(y))]\n",
    "    full_data += data\n",
    "sales_num_df = pd.DataFrame(full_data, columns=['x','y','product','brand','ID'])\n",
    "\n",
    "def pre(x):\n",
    "    if x == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "sales_num = sales_num_df['y'].apply(pre)\n",
    "def post(x):\n",
    "    if x == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# 일일 개당 판매가 = 일일 총판매액/일일 총판매량\n",
    "price['y1'] = price['y']/sales_num\n",
    "price['y1'] = price['y1'].apply(post)\n",
    "\n",
    "# 결측치 처리 : forward fill --> backward fill\n",
    "for i in tqdm(range(28894)):\n",
    "    price.iloc[i*479:(i+1)*479,5] = price.iloc[i*479:(i+1)*479,5].fillna(method='ffill')\n",
    "    price.iloc[i*479:(i+1)*479,5] = price.iloc[i*479:(i+1)*479,5].fillna(method='bfill')\n",
    "\n",
    "\n",
    "# 'ID'별 판매가율 = 일일 개당 판매가/해당 'ID'의 최대 개당 판매가\n",
    "div_price = pd.DataFrame()\n",
    "div_list = []\n",
    "def s2i(x):\n",
    "    return int(x.split(\"_\")[1])\n",
    "price1 = price.copy()\n",
    "price1['ID'] = price1['ID'].apply(s2i)\n",
    "\n",
    "\n",
    "div_price = pd.DataFrame()\n",
    "div_list = []\n",
    "def s2i(x):\n",
    "    return int(x.split(\"_\")[1])\n",
    "price1 = price.copy()\n",
    "price1['ID'] = price1['ID'].apply(s2i)\n",
    "\n",
    "\n",
    "div_price = pd.DataFrame()\n",
    "for i in tqdm(range(28894)):\n",
    "    original_price = price1[price1['ID']==i]['y1']\n",
    "    max_price = original_price.max()\n",
    "    div = pd.DataFrame((np.array(original_price)/(max_price)).tolist())\n",
    "    div_price = pd.concat([div_price,div],ignore_index=True)\n",
    "    \n",
    "price['y1'] = div_price\n",
    "\n",
    "price.to_csv(\"./price.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 브랜드 언급량 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#브랜드 키워드 언급량 파일 불러오기\n",
    "df = pd.read_csv(\"./brand_keyword_cnt.csv\")\n",
    "\n",
    "# 날짜 열들만 선택\n",
    "date_columns = df.columns[1:]\n",
    "\n",
    "# 날짜별로 median 값을 계산\n",
    "median_values = df[date_columns].median()\n",
    "\n",
    "# 모든 날짜에 대해 median 미만의 값을 가지는 브랜드명을 추출하고 그 브랜드의 값은 전부 0으로 변경\n",
    "selected_brands = []\n",
    "for brand in df['브랜드']:\n",
    "    brand_values = df[df['브랜드'] == brand][date_columns].values\n",
    "    if all(brand_values[0] < median_values):\n",
    "        df.loc[df['브랜드'] == brand, date_columns] = 0\n",
    "    else:\n",
    "        # 나머지는 MinMaxScaling을 위한 브랜드 데이터 추출\n",
    "        brand_data = df[df['브랜드'] == brand][date_columns]       \n",
    "        # MinMaxScaler를 초기화하고 변환\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_data = scaler.fit_transform(np.array(brand_data).reshape(-1,1)).reshape(1,-1)\n",
    "        # 변환된 값을 원본 데이터프레임에 업데이트\n",
    "        df.loc[df['브랜드'] == brand, date_columns] = scaled_data\n",
    "\n",
    "#데이터 병향을 위해 판매량 데이터 불러오기\n",
    "df2=pd.read_csv(\"./train2.csv\")\n",
    "\n",
    "#각 제품의 브랜드에 맞게 브랜드언급량 scaling값 대입\n",
    "df2_1=df2.iloc[:,:7]\n",
    "df3=pd.merge(df2_1,df,on=\"브랜드\")\n",
    "\n",
    "#결측치 0값 대체\n",
    "df3_1=df3.fillna(0)\n",
    "\n",
    "#csv파일 저장\n",
    "df3_1.to_csv(\"./HOT.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 판매량 0 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train2.csv\")\n",
    "\n",
    "# 판매량이 없으면 1로 labeling, 있으면 0으로 labeling\n",
    "def isnull(x):\n",
    "    if x==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df1 = train.iloc[:,7:]\n",
    "df1 = df1.transpose()\n",
    "for k in tqdm(range(len(df1.columns))):\n",
    "    df1.iloc[:,k]=df1.iloc[:,k].apply(isnull) \n",
    "\n",
    "null_index = df1.transpose()\n",
    "null_index.to_csv(\"./null_index.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 소분류별 시장 동향 예측 특성 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost train dataset : 날짜, 소분류 encoding 및 판매 지수 labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022년 XGB train dataset\n",
    "train = pd.read_csv(\"./train2.csv\")\n",
    "\n",
    "data = train.iloc[:,7:].columns\n",
    "date_df = pd.DataFrame(data)\n",
    "date_df['Date'] = pd.to_datetime(date_df.iloc[:,0])\n",
    "date_df['Year'] = date_df['Date'].dt.year\n",
    "date_df['Month'] = date_df['Date'].dt.month\n",
    "date_df['Day'] = date_df['Date'].dt.day\n",
    "\n",
    "# 월/일 encoding\n",
    "date_365 = date_df.iloc[:365,3:5] \n",
    "date_365_df = date_365.copy()\n",
    "for i in tqdm(range(28894-1)):\n",
    "    date_365_df = pd.concat([date_365_df,date_365],ignore_index=True) \n",
    "\n",
    "# 요일 encoding\n",
    "week = ([6,7,1,2,3,4,5]*52+[6])*28894 \n",
    "date_365_df['Week'] = pd.DataFrame(week) \n",
    "\n",
    "holiday_list1 = ['2022-01-01','2022-03-01','2022-03-09','2022-05-05','2022-05-08',\n",
    "                '2022-06-01','2022-06-06','2022-08-15','2022-10-03',\n",
    "                '2022-10-09','2022-10-10','2022-12-25']\n",
    "\n",
    "holiday_list2 = ['2022-01-29','2022-01-30','2022-01-31','2022-02-01','2022-02-02',\n",
    "                 '2022-09-09','2022-09-10','2022-09-11','2022-09-12']\n",
    "\n",
    "# 공휴일 1로 labeling\n",
    "selected_rows1 = pd.DataFrame() \n",
    "for i in holiday_list1:\n",
    "    selected_rows1 = pd.concat([selected_rows1,date_df[date_df[0]==i]])\n",
    "    selected_rows1['Holiday'] = 1\n",
    "\n",
    "# 설날/추석 연휴 2로 labeling\n",
    "selected_rows2 = pd.DataFrame()\n",
    "for i in holiday_list2:\n",
    "    selected_rows2 = pd.concat([selected_rows2,date_df[date_df[0]==i]])\n",
    "    selected_rows2['Holiday'] = 2\n",
    "\n",
    "selected_rows = pd.concat([selected_rows1, selected_rows2],ignore_index=True)\n",
    "selected_rows = selected_rows.iloc[:,3:6]\n",
    "result = pd.merge(date_365_df, selected_rows,how='left')\n",
    "\n",
    "# 평일 0으로 labeling\n",
    "result['Holiday'] = result['Holiday'].fillna(0)\n",
    "\n",
    "# 소분류 encoding\n",
    "k = train['소분류']\n",
    "for i in range(364):\n",
    "    k = pd.concat([k,train['소분류']],axis=1)\n",
    "k = k.values.flatten()\n",
    "k = pd.DataFrame(k)\n",
    "train_XGB_df = pd.concat([result,k],ignore_index=True,axis=1) \n",
    "\n",
    "train_XGB_df = train_XGB_df.rename(columns={0:'Month',1:'Day',2:'Week',3:'Holiday',4:'소분류'})\n",
    "\n",
    "def split_sub(x):\n",
    "    return int(x.split(\"-\")[2])    \n",
    "train_XGB_df['소분류'] = train_XGB_df['소분류'].apply(split_sub)\n",
    "\n",
    "# 판매 지수 labeling\n",
    "train_365 = train.iloc[:,7:372]\n",
    "for i in tqdm(range(28894)):\n",
    "    top = train_365.iloc[i,:].quantile(0.80)\n",
    "    bot = train_365.iloc[i,:].quantile(0.20)\n",
    "    for k in range(365):\n",
    "        x = train_365.iloc[i,k]\n",
    "        if x > top:\n",
    "            train_365.iloc[i,k] = 1\n",
    "        elif x <= bot:\n",
    "            train_365.iloc[i,k] = 2\n",
    "        else:\n",
    "            train_365.iloc[i,k] = 0\n",
    "\n",
    "train_XGB_df['y'] = pd.DataFrame(train_365.values.flatten())\n",
    "\n",
    "train_XGB_df.to_csv(\"./train_XGB_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost inference dataset : 날짜, 소분류 encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023/4/24까지 XGB infer dataset\n",
    "train = pd.read_csv(\"./train2.csv\")\n",
    "\n",
    "data = train.iloc[:,7:].columns\n",
    "date_df = pd.DataFrame(data)\n",
    "date_df['Date'] = pd.to_datetime(date_df.iloc[:,0])\n",
    "date_df['Year'] = date_df['Date'].dt.year\n",
    "date_df['Month'] = date_df['Date'].dt.month\n",
    "date_df['Day'] = date_df['Date'].dt.day\n",
    "\n",
    "# 월/일 encoding\n",
    "date_479 = date_df.iloc[:479,3:5] \n",
    "date_479_df = date_479.copy()\n",
    "for i in tqdm(range(28894-1)):\n",
    "    date_479_df = pd.concat([date_479_df,date_479],ignore_index=True)\n",
    "\n",
    "# 요일 encoding\n",
    "week = ([6,7,1,2,3,4,5]*68+[6,7,1])*28894 \n",
    "date_479_df['Week'] = pd.DataFrame(week) \n",
    "\n",
    "holiday_list1 = ['2022-01-01','2022-03-01','2022-03-09','2022-05-05','2022-05-08',\n",
    "                '2022-06-01','2022-06-06','2022-08-15','2022-10-03','2022-10-09',\n",
    "                '2022-10-10','2022-12-25','2023-01-01','2023-03-01']\n",
    "holiday_list2 = ['2022-01-29','2022-01-30','2022-01-31','2022-02-01','2022-02-02',\n",
    "                 '2022-09-09','2022-09-10','2022-09-11','2022-09-12','2023-01-21',\n",
    "                 '2023-01-22','2023-01-23','2023-01-24']\n",
    "\n",
    "# 공휴일 1로 labeling\n",
    "selected_rows1 = pd.DataFrame() \n",
    "for i in holiday_list1:\n",
    "    selected_rows1 = pd.concat([selected_rows1,date_df[date_df[0]==i]])\n",
    "    selected_rows1['Holiday'] = 1\n",
    "\n",
    "# 설날/추석 연휴 2로 labeling\n",
    "selected_rows2 = pd.DataFrame()\n",
    "for i in holiday_list2:\n",
    "     selected_rows2 = pd.concat([selected_rows2,date_df[date_df[0]==i]])\n",
    "     selected_rows2['Holiday'] = 2\n",
    "\n",
    "selected_rows = pd.concat([selected_rows1, selected_rows2],ignore_index=True)\n",
    "selected_rows.drop(columns=['Date','Year','Month','Day'],inplace=True)\n",
    "resultt = pd.merge(date_df,selected_rows,on=0,how='left')\n",
    "resultt = resultt.iloc[:,3:]\n",
    "\n",
    "# 평일 0으로 labeling\n",
    "resultt['Holiday'] = resultt['Holiday'].fillna(0)\n",
    "\n",
    "resultt = resultt['Holiday']\n",
    "resulttt = resultt.copy()\n",
    "for k in tqdm(range(28894-1)):\n",
    "    resultt = pd.concat([resultt,resulttt],ignore_index=True)\n",
    "\n",
    "date_479_df['Holiday'] = resultt\n",
    "\n",
    "# 소분류 encoding\n",
    "m = train['소분류']\n",
    "for i in tqdm(range(479-1)):\n",
    "    m = pd.concat([m,train['소분류']],axis=1)\n",
    "m = m.values.flatten()\n",
    "m = pd.DataFrame(m)\n",
    "infer_XGB_479_df = pd.concat([date_479_df,m],ignore_index=True,axis=1) \n",
    "\n",
    "infer_XGB_479_df = infer_XGB_479_df.rename(columns={0:'Month',1:'Day',2:'Week',3:'Holiday',4:'소분류'})\n",
    "\n",
    "def split_sub(x):\n",
    "    return int(x.split(\"-\")[2])    \n",
    "infer_XGB_479_df['소분류'] = infer_XGB_479_df['소분류'].apply(split_sub)\n",
    "\n",
    "infer_XGB_479_df.to_csv(\"./infer_XGB_479_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023/5/15까지 XGB infer dataset\n",
    "train = pd.read_csv(\"./train2.csv\")\n",
    "\n",
    "data = train.iloc[:,7:].columns\n",
    "date_df = pd.DataFrame(data)\n",
    "date_df['Date'] = pd.to_datetime(date_df.iloc[:,0])\n",
    "date_df['Year'] = date_df['Date'].dt.year\n",
    "date_df['Month'] = date_df['Date'].dt.month\n",
    "date_df['Day'] = date_df['Date'].dt.day\n",
    "\n",
    "# 월/일 encoding\n",
    "date_479 = date_df.iloc[:479,3:5] \n",
    "date_500 = pd.concat([date_479,date_479.iloc[114:135,:]],ignore_index=True) \n",
    "date_500_df = date_500.copy() \n",
    "for i in tqdm(range(28894-1)):\n",
    "    date_500_df = pd.concat([date_500_df,date_500],ignore_index=True)\n",
    "\n",
    "# 요일 encoding\n",
    "week = ([6,7,1,2,3,4,5]*71+[6,7,1])*28894 \n",
    "date_500_df['Week'] = pd.DataFrame(week)\n",
    "\n",
    "# 공휴일 labeling\n",
    "infer_XGB_500 = infer_XGB_479_df.iloc[:479,2]\n",
    "result = pd.DataFrame(np.array([0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]))\n",
    "infer_500_holiday = pd.concat([infer_XGB_500,result])\n",
    "infer_500_holiday_data = infer_500_holiday.copy()\n",
    "for i in tqdm(range(28894-1)):\n",
    "    infer_500_holiday = pd.concat([infer_500_holiday,infer_500_holiday_data],ignore_index=True)\n",
    "\n",
    "date_500_df['Holiday'] = infer_500_holiday\n",
    "\n",
    "# 소분류 encoding\n",
    "m = train['소분류']\n",
    "for i in range(500-1):\n",
    "    m = pd.concat([m,train['소분류']],axis=1)\n",
    "m = m.values.flatten()\n",
    "m = pd.DataFrame(m)\n",
    "infer_XGB_500_df = pd.concat([date_500_df,m],ignore_index=True,axis=1) \n",
    "\n",
    "infer_XGB_500_df = infer_XGB_500_df.rename(columns={0:'Month',1:'Day',2:'Week',3:'Holiday',4:'소분류'})\n",
    "\n",
    "def split_sub(x):\n",
    "    return int(x.split(\"-\")[2])    \n",
    "infer_XGB_500_df['소분류'] = infer_XGB_500_df['소분류'].apply(split_sub)\n",
    "\n",
    "infer_XGB_500_df.to_csv(\"./infer_XGB_500_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost train dataset을 이용하여 소분류별 XGBoost model 학습 및 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train2.csv\")\n",
    "\n",
    "# 소분류 기준으로 groupby\n",
    "train_XGB = train_XGB_df.groupby('소분류')\n",
    "train_XGB_gb_df = [group_df for _, group_df in train_XGB]\n",
    "\n",
    "infer_XGB_479 = infer_XGB_479_df.groupby('소분류')\n",
    "infer_XGB_479_gb_df = [group_df for _, group_df in infer_XGB_479]\n",
    "\n",
    "infer_XGB_500 = infer_XGB_500_df.groupby('소분류')\n",
    "infer_XGB_500_gb_df = [group_df for _, group_df in infer_XGB_500]\n",
    "\n",
    "proba_XGB_479_df = pd.DataFrame()\n",
    "proba_XGB_500_df = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(53)):\n",
    "    # up-sampling\n",
    "    X1 = train_XGB_gb_df[i][train_XGB_gb_df[i]['y']==2].iloc[:,:-2]\n",
    "    X2 = train_XGB_gb_df[i][train_XGB_gb_df[i]['y']==0].iloc[:,:-2]\n",
    "    X3 = train_XGB_gb_df[i][train_XGB_gb_df[i]['y']==1].iloc[:,:-2]\n",
    "    y1 = train_XGB_gb_df[i][train_XGB_gb_df[i]['y']==2].iloc[:,-1]\n",
    "    y2 = train_XGB_gb_df[i][train_XGB_gb_df[i]['y']==0].iloc[:,-1]\n",
    "    y3 = train_XGB_gb_df[i][train_XGB_gb_df[i]['y']==1].iloc[:,-1]\n",
    "    \n",
    "    if train_XGB_gb_df[i]['y'].value_counts()[2] == 0:\n",
    "        vm = 0\n",
    "    else:\n",
    "        vm = round(train_XGB_gb_df[i]['y'].value_counts().max() / train_XGB_gb_df[i]['y'].value_counts()[2])\n",
    "\n",
    "    if train_XGB_gb_df[i]['y'].value_counts()[0] == 0:\n",
    "        vz = 0\n",
    "    else:\n",
    "        vz = round(train_XGB_gb_df[i]['y'].value_counts().max() / train_XGB_gb_df[i]['y'].value_counts()[0])\n",
    "    \n",
    "    if train_XGB_gb_df[i]['y'].value_counts()[1] == 0:\n",
    "        vo = 0\n",
    "    else:\n",
    "        vo = round(train_XGB_gb_df[i]['y'].value_counts().max() / train_XGB_gb_df[i]['y'].value_counts()[1])\n",
    "    \n",
    "    if vm == 1:\n",
    "        X = X1\n",
    "        y = y1\n",
    "        for j in range(vz):\n",
    "            X = pd.concat([X,X2])\n",
    "            y = pd.concat([y,y2])\n",
    "        for k in range(vo):\n",
    "            X = pd.concat([X,X3])\n",
    "            y = pd.concat([y,y3])\n",
    "    elif vz == 1:\n",
    "        X = X2\n",
    "        y = y2\n",
    "        for j in range(vm):\n",
    "            X = pd.concat([X,X1])\n",
    "            y = pd.concat([y,y1])\n",
    "        for k in range(vo):\n",
    "            X = pd.concat([X,X3])\n",
    "            y = pd.concat([y,y3])\n",
    "    elif vo == 1:\n",
    "        X = X3\n",
    "        y = y3\n",
    "        for j in range(vm):\n",
    "            X = pd.concat([X,X1])\n",
    "            y = pd.concat([y,y1])\n",
    "        for k in range(vz):\n",
    "            X = pd.concat([X,X2])\n",
    "            y = pd.concat([y,y2])\n",
    "\n",
    "    # 소분류별 XGBoost model train    \n",
    "    clf = xgb.XGBClassifier(random_state=0)\n",
    "    clf.fit(X,y)\n",
    "    \n",
    "    # 소분류별 XGBoost model inference(2023/4/24까지)\n",
    "    proba_list = clf.predict_proba(infer_XGB_479_gb_df[i].iloc[:479,:-1])\n",
    "    pl_df = pd.DataFrame(proba_list, columns=['0 proba', '1 proba', '-1 proba'])\n",
    "    \n",
    "    pl_df['peak_index'] = pl_df['1 proba']-pl_df['-1 proba']\n",
    "    del pl_df['0 proba']\n",
    "    del pl_df['1 proba']\n",
    "    del pl_df['-1 proba']\n",
    "    pl_df = pl_df.transpose()\n",
    "    \n",
    "    if i < 9:\n",
    "        pl_df['소분류'] =  f\"B002-C003-000{i+1}\"\n",
    "    else:\n",
    "        pl_df['소분류'] =  f\"B002-C003-00{i+1}\"\n",
    "    proba_XGB_479_df = pd.concat([proba_XGB_479_df,pl_df])\n",
    "\n",
    "    # 소분류별 XGBoost model inference(2023/5/15까지)\n",
    "    proba_listt = clf.predict_proba(infer_XGB_500_gb_df[i].iloc[:500,:-1])\n",
    "    pll_df = pd.DataFrame(proba_listt, columns=['0 proba', '1 proba', '-1 proba'])\n",
    "    \n",
    "    pll_df['peak_index'] = pll_df['1 proba']-pll_df['-1 proba']\n",
    "    del pll_df['0 proba']\n",
    "    del pll_df['1 proba']\n",
    "    del pll_df['-1 proba']\n",
    "    pll_df = pll_df.transpose()\n",
    "    \n",
    "    if i < 9:\n",
    "        pll_df['소분류'] =  f\"B002-C003-000{i+1}\"\n",
    "    else:\n",
    "        pll_df['소분류'] =  f\"B002-C003-00{i+1}\"\n",
    "    proba_XGB_500_df = pd.concat([proba_XGB_500_df,pll_df])\n",
    "\n",
    "pi = pd.merge(train['소분류'],proba_XGB_479_df,how='left',on='소분류')\n",
    "peak_424 = pd.DataFrame(pi.iloc[:,1:].values.flatten())\n",
    "peak_424.to_csv(\"./peak_index.csv\",index=False)\n",
    "\n",
    "pii = pd.merge(train['소분류'],proba_XGB_500_df,how='left',on='소분류')\n",
    "peak_515 = pd.DataFrame(pii.iloc[:,1:].values.flatten())\n",
    "peak_515.to_csv(\"./peak_index_500.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE':90, \n",
    "    'PREDICT_SIZE':21, \n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':1e-4,\n",
    "    'BATCH_SIZE':1024,\n",
    "    'SEED':41\n",
    "}\n",
    "\n",
    "# Seed값 고정 함수\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 대분류별 train/inference dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sub(x):\n",
    "    return int(x.split(\"_\")[1])  \n",
    "\n",
    "temp = pd.read_csv('./train2.csv')\n",
    "temp['ID'] = temp['ID'].apply(split_sub)\n",
    "temp.to_csv('./train2.csv', index=False)\n",
    "\n",
    "temp = pd.read_csv('./sales2.csv')\n",
    "temp['ID'] = temp['ID'].apply(split_sub)\n",
    "temp.to_csv('./sales2.csv', index=False)\n",
    "\n",
    "temp = pd.read_csv('./HOT.csv')\n",
    "temp['ID'] = temp['ID'].apply(split_sub)\n",
    "temp.to_csv('./HOT.csv', index=False)\n",
    "\n",
    "temp = pd.read_csv('./price.csv')\n",
    "temp['ID'] = temp['ID'].apply(split_sub)\n",
    "temp.to_csv('./price.csv', index=False)\n",
    "\n",
    "temp = pd.read_csv('./train_MDS_train1ver.csv')\n",
    "temp['ID'] = temp['ID'].apply(split_sub)\n",
    "temp.to_csv('./train_MDS_train1ver.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./train2.csv').drop(columns=['제품', 'ID','브랜드'])\n",
    "train_data1 = pd.read_csv(\"./train_MDS_train1ver.csv\")\n",
    "\n",
    "n_bigcat_list = train_data.value_counts('대분류').sort_index().to_list()\n",
    "index1 = (train_data['대분류']=='B002-C001-0001')\n",
    "index2 = (train_data['대분류']=='B002-C001-0002')\n",
    "index3 = (train_data['대분류']=='B002-C001-0003')\n",
    "index4 = (train_data['대분류']=='B002-C001-0004')\n",
    "index5 = (train_data['대분류']=='B002-C001-0005')\n",
    "train_data['대분류'] = train_data1['대분류']\n",
    "train_data['중분류'] = train_data1['중분류']\n",
    "train_data['소분류'] = train_data1['소분류']\n",
    "train_data['쇼핑몰'] = train_data1['쇼핑몰']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(train_data, index_list):\n",
    "    df2 = pd.read_csv(\"./price.csv\")\n",
    "    del df2['x']\n",
    "    del df2['y']\n",
    "    del df2['product']\n",
    "    del df2['brand']\n",
    "\n",
    "    df3 = pd.read_csv(\"./HOT.csv\").drop(columns=[\"ID\",\"제품\",\"대분류\",\"중분류\",\"소분류\",\"브랜드\",\"쇼핑몰\"])\n",
    "    df4 = pd.read_csv(\"./null_index.csv\")\n",
    "    df5 = pd.read_csv(\"./peak_index.csv\")\n",
    "    hot_list = []\n",
    "    null_list = []\n",
    "    price_list = []\n",
    "    peak_list = []\n",
    "    for j in tqdm(range(len(index_list[index_list==True].index))):\n",
    "        exec(f\"price_list += df2[df2['ID']==index_list[{j}]]['y1'].tolist()\")\n",
    "        exec(f\"hot_list += df3.iloc[index_list[index_list==True].index[{j}],:].tolist()\")\n",
    "        exec(f\"null_list += df4.iloc[index_list[index_list==True].index[{j}],:].tolist()\")\n",
    "        exec(f\"peak_list += df5.iloc[index_list[index_list==True].index[{j}]*479:(index_list[index_list==True].index[{j}]+1)*479].to_numpy().reshape(-1).tolist()\")\n",
    "\n",
    "    return hot_list, peak_list, null_list, price_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_data(data, other_data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "\n",
    "    hot_data, peak_data, null_data, price_data = other_data\n",
    "\n",
    "    num_rows = len(data) \n",
    "    window_size = train_size + predict_size \n",
    "    input_data = np.empty((num_rows * (len(data.columns) - 4 - window_size + 1), train_size, 9))\n",
    "\n",
    "    target_data = np.empty((num_rows * (len(data.columns) - 4 - window_size + 1), predict_size))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, 4:]) \n",
    "        hot_data_i = np.array(hot_data[i*479:(i+1)*479])\n",
    "        peak_data_i = np.array(peak_data[i*479:(i+1)*479])\n",
    "        null_data_i = np.array(null_data[i*479:(i+1)*479])\n",
    "        price_data_i = np.array(price_data[i*479:(i+1)*479])\n",
    "        for j in range(len(data.columns) - 4 - window_size + 1): \n",
    "            window = sales_data[j : j + window_size]\n",
    "            \n",
    "            temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size])).tolist()\n",
    "            \n",
    "            temp_list2 = []\n",
    "            for k in range(len(temp_data)):\n",
    "                temp_list = temp_data[k]\n",
    "                temp_list += [hot_data_i[j+k],peak_data_i[j+k],null_data_i[j+k],price_data_i[j+k]]\n",
    "                \n",
    "                temp_list2.append(temp_list)\n",
    "            temp_data = temp_list2\n",
    "            \n",
    "            input_data[i * (len(data.columns) - 4 - window_size + 1) + j] = temp_data \n",
    "            target_data[i * (len(data.columns) - 4 - window_size + 1) + j] = window[train_size:] \n",
    "\n",
    "    return input_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대분류별 encoder input window dataset과 decoder output window dataset 생성\n",
    "train_input1, train_target1 = make_train_data(train_data[index1], make_data(train_data, index1))\n",
    "train_input2, train_target2 = make_train_data(train_data[index2], make_data(train_data, index2))\n",
    "train_input3, train_target3 = make_train_data(train_data[index3], make_data(train_data, index3))\n",
    "train_input4, train_target4 = make_train_data(train_data[index4], make_data(train_data, index4))\n",
    "train_input5, train_target5 = make_train_data(train_data[index5], make_data(train_data, index5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_proba_data(data, other_data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    " \n",
    "    hot_data, peak_data, null_data, price_data = other_data\n",
    "    \n",
    "    num_rows = len(data) \n",
    "    window_size = train_size + predict_size \n",
    "    target_data = np.empty((num_rows * (len(data.columns) - 4 - window_size + 1), predict_size))\n",
    "\n",
    "    for i in tqdm(range(num_rows)): \n",
    "        peak_data_i = np.array(peak_data[i*479:(i+1)*479])\n",
    "        for j in range(len(data.columns) - 4 - window_size + 1): \n",
    "            window = peak_data_i[j : j + window_size]\n",
    "            target_data[i * (len(data.columns) - 4 - window_size + 1) + j] = window[train_size:] \n",
    "\n",
    "    return target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대분류별 decoder input window dataset 생성\n",
    "proba_data1 = make_proba_data(train_data[index1], make_data(train_data, index1))\n",
    "proba_data2 = make_proba_data(train_data[index2], make_data(train_data, index2))\n",
    "proba_data3 = make_proba_data(train_data[index3], make_data(train_data, index3))\n",
    "proba_data4 = make_proba_data(train_data[index4], make_data(train_data, index4))\n",
    "proba_data5 = make_proba_data(train_data[index5], make_data(train_data, index5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset2(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = torch.tensor(X).type(torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.Tensor(self.X[index]).type(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict_data(data, other_data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n",
    "  \n",
    "    num_rows = len(data)\n",
    "    hot_data, peak_data, null_data, price_data = other_data\n",
    "  \n",
    "    input_data = np.empty((num_rows, train_size, 9))\n",
    "\n",
    "    for i in tqdm(range(num_rows)):\n",
    "        encode_info = np.array(data.iloc[i, :4])\n",
    "        sales_data = np.array(data.iloc[i, -train_size:])\n",
    "        hot_data_i = np.array(hot_data[(i+1)*479-train_size:(i+1)*479])\n",
    "        peak_data_i = np.array(peak_data[(i+1)*479-train_size:(i+1)*479])\n",
    "        null_data_i = np.array(null_data[(i+1)*479-train_size:(i+1)*479])\n",
    "        price_data_i = np.array(price_data[(i+1)*479-train_size:(i+1)*479])\n",
    "\n",
    "        window = sales_data[-train_size : ]\n",
    "        temp_data = np.column_stack((np.tile(encode_info, (train_size, 1)), window[:train_size])).tolist()\n",
    "        temp_list2 = []\n",
    "        for k in range(len(temp_data)):\n",
    "            temp_list = temp_data[k]\n",
    "            temp_list += [hot_data_i[k],peak_data_i[k],null_data_i[k],price_data_i[k]]\n",
    "            \n",
    "            temp_list2.append(temp_list)\n",
    "        temp_data = temp_list2\n",
    "        input_data[i] = temp_data\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder inference input window dataset 생성\n",
    "test_input = make_predict_data(train_data, make_data(train_data, train_data['대분류'].notna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X).type(torch.float32)\n",
    "        self.Y = torch.tensor(Y).type(torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]).type(torch.float32), torch.Tensor(self.Y[index]).type(torch.float32)\n",
    "        return torch.Tensor(self.X[index]).type(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM + Regressor 모델\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, input_size=9, hidden_size=512, output_size=CFG['PREDICT_SIZE']):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size//2, output_size)\n",
    "        )\n",
    "\n",
    "        self.actv = nn.ReLU()\n",
    "        self.re = nn.Sequential(\n",
    "            nn.Linear(output_size*2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size//2, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x1):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size, x.device)\n",
    "\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        last_output = lstm_out[:, -1, :]\n",
    "\n",
    "        output = self.actv(self.fc(last_output))\n",
    "        output = output.squeeze(1)\n",
    "        x1 = output * x1\n",
    "        x2 = torch.cat((output,x1),dim=1)\n",
    "        output = self.actv(self.re(x2))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "       \n",
    "        return (torch.zeros(1, batch_size, self.hidden_size, device=device).type(torch.float32),\n",
    "                torch.zeros(1, batch_size, self.hidden_size, device=device).type(torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSFA_t(pred, target,device):\n",
    "\n",
    "    PSFA = 1\n",
    "    ids = range(len(target))\n",
    "    for day in range(21): \n",
    "        total_sell = torch.tensor([]).to(device)\n",
    "        pred_values = torch.tensor([]).to(device)\n",
    "        target_values = torch.tensor([]).to(device)\n",
    "        for index in ids: \n",
    "            pred_values=torch.cat([pred_values,pred[index][day].reshape(1)]).to(device) \n",
    "            target_values=torch.cat([target_values,target[index][day].reshape(1)]).to(device) \n",
    "        total_sell = torch.sum(target_values).to(device)  \n",
    "        \n",
    "        pred_values = torch.nan_to_num(pred_values).to(device)\n",
    "        pred_values = torch.maximum(torch.zeros_like(pred_values),pred_values).to(device)\n",
    "        denominator = torch.maximum(target_values, pred_values).to(device) \n",
    "\n",
    "        diffs = torch.where(denominator!=0, torch.abs(target_values - pred_values)/ denominator, 0).to(device)\n",
    "\n",
    "        if total_sell != 0:\n",
    "            sell_weights = target_values / total_sell  \n",
    "        else:\n",
    "            sell_weights = torch.zeros_like(target_values).to(device) \n",
    "\n",
    "        if not torch.isnan(diffs).any():  \n",
    "            PSFA -= torch.sum(diffs * sell_weights).to(device) / (21)\n",
    "\n",
    "\n",
    "    return PSFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSFA_v(pred, target):\n",
    "    \n",
    "    PSFA = 1\n",
    "    ids = range(len(target))\n",
    "    for day in range(21): \n",
    "        total_sell = np.array([])\n",
    "        pred_values = np.array([])\n",
    "        target_values = np.array([])\n",
    "        for index in ids: \n",
    "            pred_values=np.concatenate([pred_values,pred[index][day].reshape(1)])\n",
    "            target_values=np.concatenate([target_values,target[index][day].reshape(1)]) \n",
    "        total_sell = np.sum(target_values)  \n",
    "        pred_values = np.nan_to_num(pred_values)\n",
    "        pred_values = np.maximum(np.zeros_like(pred_values),pred_values)\n",
    "        denominator = np.maximum(target_values, pred_values) \n",
    "\n",
    "        diffs = np.where(denominator!=0, np.abs(target_values - pred_values) / denominator, 0)\n",
    "\n",
    "        if total_sell != 0:\n",
    "            sell_weights = target_values / total_sell  \n",
    "        else:\n",
    "            sell_weights = np.zeros_like(target_values) \n",
    "\n",
    "        if not np.isnan(diffs).any():  \n",
    "            PSFA -= np.sum(diffs * sell_weights) / (21) \n",
    "\n",
    "\n",
    "    return PSFA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, n_bigcat_list, train_input1, train_target1, train_input2, train_target2, train_input3, train_target3, train_input4, train_target4, train_input5, train_target5, proba_data1, proba_data2, proba_data3, proba_data4, proba_data5):\n",
    "    val_accuracy = 0\n",
    "    for timeitem in [114, 368]:\n",
    "        model.eval()\n",
    "        batch_n_bigcat_list = (np.array(n_bigcat_list)).astype(int)\n",
    "        batch_time_list = random.sample([timeitem], k = 1)\n",
    "        for k in range(100):\n",
    "            for i in range(1,6): \n",
    "                exec(f\"batch_index_{i}_bigcat_{k} = (random.sample(range(n_bigcat_list[{i-1}]*{k}//100,n_bigcat_list[{i-1}]*(k+1)//100), k=len(range(n_bigcat_list[{i-1}]*{k}//100,n_bigcat_list[{i-1}]*(k+1)//100))))\")\n",
    "\n",
    "        for k in range(100):\n",
    "            for i in range(1,6):\n",
    "\n",
    "                batch_index = eval(f\"batch_index_{i}_bigcat_{k}[0]\")\n",
    "                exec(f\"batch_train_input{i}_{k} = train_input{i}[{batch_index*369}:{(batch_index+1)*369}][batch_time_list]\")\n",
    "                exec(f\"batch_train_target{i}_{k} = train_target{i}[{batch_index*369}:{(batch_index+1)*369}][batch_time_list]\")\n",
    "                exec(f\"batch_proba_data{i}_{k} = proba_data{i}[{batch_index*369}:{(batch_index+1)*369}][batch_time_list]\")\n",
    "                for batch_index in eval(f\"batch_index_{i}_bigcat_{k}[1:]\"):\n",
    "                    temp1 = eval(f\"train_input{i}[{batch_index*369}:{(batch_index+1)*369}]\")\n",
    "                    temp2 = eval(f\"train_target{i}[{batch_index*369}:{(batch_index+1)*369}]\")\n",
    "                    temp3 = eval(f\"proba_data{i}[{batch_index*369}:{(batch_index+1)*369}]\")\n",
    "                    exec(f\"batch_train_input{i}_{k} = np.concatenate((batch_train_input{i}_{k}, temp1[batch_time_list]))\")\n",
    "                    exec(f\"batch_train_target{i}_{k} = np.concatenate((batch_train_target{i}_{k}, temp2[batch_time_list]))\")\n",
    "                    exec(f\"batch_proba_data{i}_{k} = np.concatenate((batch_proba_data{i}_{k}, temp3[batch_time_list]))\")\n",
    "                exec(f\"train_dataset{i}_{k} = CustomDataset(batch_train_input{i}_{k}, batch_train_target{i}_{k})\")\n",
    "                exec(f\"proba_dataset{i}_{k} = CustomDataset2(batch_proba_data{i}_{k})\")\n",
    "\n",
    "        total_accuracy = 0\n",
    "        total_loss = 0\n",
    "        for i in range(1,6): \n",
    "            pred = 0\n",
    "            target = 0\n",
    "            for k in range(100):\n",
    "                X = eval(f\"train_dataset{i}_{k}.X.to(device)\")\n",
    "                Y = eval(f\"train_dataset{i}_{k}.Y.to(device)\")\n",
    "                X1 = eval(f\"proba_dataset{i}_{k}.X.to(device)\")\n",
    "                output = model(X, X1)\n",
    "\n",
    "                pred1 = torch.nan_to_num(output)\n",
    "                pred1 = torch.maximum(torch.zeros_like(pred1),pred1)\n",
    "                target1 = Y\n",
    "                pred1 = pred1.detach().cpu().numpy()\n",
    "                target1 = target1.detach().cpu().numpy()\n",
    "                if k == 0:\n",
    "                    pred = pred1\n",
    "                    target = target1\n",
    "                else:\n",
    "                    pred = np.concatenate((pred,pred1))\n",
    "                    target = np.concatenate((target,target1))\n",
    "            total_accuracy += PSFA_v(pred, target)\n",
    "        val_accuracy += (total_accuracy)/5\n",
    "    val_accuracy /= 2\n",
    "    return val_accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, n_bigcat_list, train_input1, train_target1, train_input2, train_target2, train_input3, train_target3, train_input4, train_target4, train_input5, train_target5, proba_data1, proba_data2, proba_data3, proba_data4, proba_data5, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    best_accuracy = 0\n",
    "    best_loss = 1000\n",
    "    best_model = None\n",
    "    accuracy_list = []\n",
    "\n",
    "    # train 시, 이상치 제거\n",
    "    for i in range(1,6):\n",
    "        df = pd.read_csv(\"./train2.csv\")[pd.read_csv(\"./train2.csv\")['대분류']==f'B002-C001-000{i}'].reset_index(drop=True)\n",
    "        df1 = df.iloc[:,7:].replace(0,np.nan)\n",
    "\n",
    "        exec(f\"not_in{i}=set(list(df1.isna().sum(axis=1)[df1.isna().sum(axis=1)>=420].index))\")\n",
    "\n",
    "    for epoch in range(1, 71):\n",
    "\n",
    "        model.train()\n",
    "        train_accuracy = []\n",
    "        train_loss = []\n",
    "        train_mae = []\n",
    "\n",
    "        # 대분류별 10% batch로 뽑음\n",
    "        id_ratio = 0.1\n",
    "        for inner_epoch in tqdm(range(30)):\n",
    "\n",
    "            batch_n_bigcat_list = (np.array(n_bigcat_list) * id_ratio+1).astype(int)\n",
    "            \n",
    "            # 시점 sampling\n",
    "            if inner_epoch % 3 == 0:\n",
    "                batch_time_list = random.sample(list(range(114))+list(range(115,123)), k = 1)\n",
    "            elif inner_epoch % 3 == 1:\n",
    "                batch_time_list = random.sample(range(123,246), k = 1)\n",
    "            else:\n",
    "                batch_time_list = random.sample(range(246,368), k = 1)\n",
    "\n",
    "            for i in range(1,6):\n",
    "                exec(f\"batch_index_{i}_bigcat = (random.sample(list(set(range(n_bigcat_list[{i-1}]))-not_in{i}), k=min(batch_n_bigcat_list[{i-1}], len(list(set(range(n_bigcat_list[{i-1}]))-not_in{i})))))\")\n",
    "\n",
    "            for i in range(1,6):\n",
    "                batch_index = eval(f\"batch_index_{i}_bigcat[0]\")\n",
    "                exec(f\"batch_train_input{i} = train_input{i}[{batch_index*369}:{(batch_index+1)*369}][batch_time_list]\")\n",
    "                exec(f\"batch_train_target{i} = train_target{i}[{batch_index*369}:{(batch_index+1)*369}][batch_time_list]\")\n",
    "                exec(f\"batch_proba_data{i} = proba_data{i}[{batch_index*369}:{(batch_index+1)*369}][batch_time_list]\")\n",
    "                for batch_index in eval(f\"batch_index_{i}_bigcat[1:]\"):\n",
    "                    temp1 = eval(f\"train_input{i}[{batch_index*369}:{(batch_index+1)*369}]\")\n",
    "                    temp2 = eval(f\"train_target{i}[{batch_index*369}:{(batch_index+1)*369}]\")\n",
    "                    temp3 = eval(f\"proba_data{i}[{batch_index*369}:{(batch_index+1)*369}]\")\n",
    "                    exec(f\"batch_train_input{i} = np.concatenate((batch_train_input{i}, temp1[batch_time_list]))\")\n",
    "                    exec(f\"batch_train_target{i} = np.concatenate((batch_train_target{i}, temp2[batch_time_list]))\")\n",
    "                    exec(f\"batch_proba_data{i} = np.concatenate((batch_proba_data{i}, temp3[batch_time_list]))\")\n",
    "\n",
    "                exec(f\"train_dataset{i} = CustomDataset(batch_train_input{i}, batch_train_target{i})\")\n",
    "                exec(f\"proba_dataset{i} = CustomDataset2(batch_proba_data{i})\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_accuracy = 0\n",
    "            total_loss = 0\n",
    "            for i in range(1,6): \n",
    "                # X : encoder input window \n",
    "                X = eval(f\"train_dataset{i}.X.to(device)\")\n",
    "                # Y : decoder output window\n",
    "                Y = eval(f\"train_dataset{i}.Y.to(device)\")\n",
    "                # X1 : decoder input window\n",
    "                X1 = eval(f\"proba_dataset{i}.X.to(device)\")\n",
    "                output = model(X, X1).to(device)\n",
    "\n",
    "                pred = torch.nan_to_num(output).to(device)\n",
    "                pred = torch.maximum(torch.zeros_like(pred),pred).to(device)\n",
    "                target = Y.to(device)\n",
    "                total_accuracy += PSFA_t(pred, target,device).to(device)\n",
    "\n",
    "            total_accuracy /= 5\n",
    "            (-total_accuracy).backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_accuracy.append(total_accuracy.item())\n",
    "\n",
    "        val_accuracy = validation(model, n_bigcat_list, train_input1, train_target1, train_input2, train_target2, train_input3, train_target3, train_input4, train_target4, train_input5, train_target5, proba_data1, proba_data2, proba_data3, proba_data4, proba_data5)\n",
    "\n",
    "        print(f'Epoch : [{epoch}] Train Accuracy : [{np.mean(train_accuracy):.5f}] Val Accuracy : [{val_accuracy:.5f}]')\n",
    "        torch.save(model, f\"./model{epoch}.pt\")\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_model = model\n",
    "            torch.save(model, \"./best_val_model.pt\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "infer_model = train(model, optimizer, n_bigcat_list, train_input1, train_target1, train_input2, train_target2, train_input3, train_target3, train_input4, train_target4, train_input5, train_target5, proba_data1, proba_data2, proba_data3, proba_data4, proba_data5, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset1(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = torch.tensor(X).type(torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.Tensor(self.X[index]).type(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset1(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, X, X1, device):\n",
    "    predictions = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    output = model(X, X1)\n",
    "\n",
    "    output = output.detach().cpu().numpy()\n",
    "    predictions.extend(output)\n",
    "\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_model = torch.load(\"./best_val_model.pt\")\n",
    "infer_model.eval()\n",
    "\n",
    "for i in range(10):\n",
    "    slice = len(test_dataset.X)//10\n",
    "    X = test_dataset.X.to(device)[i*slice:(i+1)*slice]\n",
    "    X1 = torch.tensor(pd.DataFrame(pd.read_csv(\"./peak_index_500.csv\").to_numpy().reshape(-1,500)).iloc[:,-21:].to_numpy()).type(torch.float32).to(device)[i*slice:(i+1)*slice]\n",
    "    exec(f\"pred{i} = inference(infer_model, X, X1, device)\")\n",
    "    exec(f\"pred{i} = np.maximum(np.zeros_like(pred{i}),pred{i})\")\n",
    "    exec(f\"pred{i} = np.round(pred{i}, 0).astype(int)\")\n",
    "\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "slice = len(test_dataset.X)//10\n",
    "for i in range(10):\n",
    "    exec(f\"submit.iloc[i*slice:(i+1)*slice,1:] = pred{i}\")\n",
    "submit.to_csv('./submit_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
